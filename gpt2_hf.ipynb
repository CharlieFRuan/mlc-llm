{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rickzhou/.conda/envs/mlc-chat-venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from mlc_chat.compiler import MODEL_PRESETS, MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight [50257, 768] float32\n",
      "transformer.wpe.weight [1024, 768] float32\n",
      "transformer.h.0.ln_1.weight [768] float32\n",
      "transformer.h.0.ln_1.bias [768] float32\n",
      "transformer.h.0.attn.c_attn.weight [2304, 768] float32\n",
      "transformer.h.0.attn.c_attn.bias [2304] float32\n",
      "transformer.h.0.attn.c_proj.weight [768, 768] float32\n",
      "transformer.h.0.attn.c_proj.bias [768] float32\n",
      "transformer.h.0.ln_2.weight [768] float32\n",
      "transformer.h.0.ln_2.bias [768] float32\n",
      "transformer.h.0.mlp.c_fc.weight [3072, 768] float32\n",
      "transformer.h.0.mlp.c_fc.bias [3072] float32\n",
      "transformer.h.0.mlp.c_proj.weight [768, 3072] float32\n",
      "transformer.h.0.mlp.c_proj.bias [768] float32\n",
      "transformer.h.1.ln_1.weight [768] float32\n",
      "transformer.h.1.ln_1.bias [768] float32\n",
      "transformer.h.1.attn.c_attn.weight [2304, 768] float32\n",
      "transformer.h.1.attn.c_attn.bias [2304] float32\n",
      "transformer.h.1.attn.c_proj.weight [768, 768] float32\n",
      "transformer.h.1.attn.c_proj.bias [768] float32\n",
      "transformer.h.1.ln_2.weight [768] float32\n",
      "transformer.h.1.ln_2.bias [768] float32\n",
      "transformer.h.1.mlp.c_fc.weight [3072, 768] float32\n",
      "transformer.h.1.mlp.c_fc.bias [3072] float32\n",
      "transformer.h.1.mlp.c_proj.weight [768, 3072] float32\n",
      "transformer.h.1.mlp.c_proj.bias [768] float32\n",
      "transformer.h.2.ln_1.weight [768] float32\n",
      "transformer.h.2.ln_1.bias [768] float32\n",
      "transformer.h.2.attn.c_attn.weight [2304, 768] float32\n",
      "transformer.h.2.attn.c_attn.bias [2304] float32\n",
      "transformer.h.2.attn.c_proj.weight [768, 768] float32\n",
      "transformer.h.2.attn.c_proj.bias [768] float32\n",
      "transformer.h.2.ln_2.weight [768] float32\n",
      "transformer.h.2.ln_2.bias [768] float32\n",
      "transformer.h.2.mlp.c_fc.weight [3072, 768] float32\n",
      "transformer.h.2.mlp.c_fc.bias [3072] float32\n",
      "transformer.h.2.mlp.c_proj.weight [768, 3072] float32\n",
      "transformer.h.2.mlp.c_proj.bias [768] float32\n",
      "transformer.h.3.ln_1.weight [768] float32\n",
      "transformer.h.3.ln_1.bias [768] float32\n",
      "transformer.h.3.attn.c_attn.weight [2304, 768] float32\n",
      "transformer.h.3.attn.c_attn.bias [2304] float32\n",
      "transformer.h.3.attn.c_proj.weight [768, 768] float32\n",
      "transformer.h.3.attn.c_proj.bias [768] float32\n",
      "transformer.h.3.ln_2.weight [768] float32\n",
      "transformer.h.3.ln_2.bias [768] float32\n",
      "transformer.h.3.mlp.c_fc.weight [3072, 768] float32\n",
      "transformer.h.3.mlp.c_fc.bias [3072] float32\n",
      "transformer.h.3.mlp.c_proj.weight [768, 3072] float32\n",
      "transformer.h.3.mlp.c_proj.bias [768] float32\n",
      "transformer.h.4.ln_1.weight [768] float32\n",
      "transformer.h.4.ln_1.bias [768] float32\n",
      "transformer.h.4.attn.c_attn.weight [2304, 768] float32\n",
      "transformer.h.4.attn.c_attn.bias [2304] float32\n",
      "transformer.h.4.attn.c_proj.weight [768, 768] float32\n",
      "transformer.h.4.attn.c_proj.bias [768] float32\n",
      "transformer.h.4.ln_2.weight [768] float32\n",
      "transformer.h.4.ln_2.bias [768] float32\n",
      "transformer.h.4.mlp.c_fc.weight [3072, 768] float32\n",
      "transformer.h.4.mlp.c_fc.bias [3072] float32\n",
      "transformer.h.4.mlp.c_proj.weight [768, 3072] float32\n",
      "transformer.h.4.mlp.c_proj.bias [768] float32\n",
      "transformer.h.5.ln_1.weight [768] float32\n",
      "transformer.h.5.ln_1.bias [768] float32\n",
      "transformer.h.5.attn.c_attn.weight [2304, 768] float32\n",
      "transformer.h.5.attn.c_attn.bias [2304] float32\n",
      "transformer.h.5.attn.c_proj.weight [768, 768] float32\n",
      "transformer.h.5.attn.c_proj.bias [768] float32\n",
      "transformer.h.5.ln_2.weight [768] float32\n",
      "transformer.h.5.ln_2.bias [768] float32\n",
      "transformer.h.5.mlp.c_fc.weight [3072, 768] float32\n",
      "transformer.h.5.mlp.c_fc.bias [3072] float32\n",
      "transformer.h.5.mlp.c_proj.weight [768, 3072] float32\n",
      "transformer.h.5.mlp.c_proj.bias [768] float32\n",
      "transformer.h.6.ln_1.weight [768] float32\n",
      "transformer.h.6.ln_1.bias [768] float32\n",
      "transformer.h.6.attn.c_attn.weight [2304, 768] float32\n",
      "transformer.h.6.attn.c_attn.bias [2304] float32\n",
      "transformer.h.6.attn.c_proj.weight [768, 768] float32\n",
      "transformer.h.6.attn.c_proj.bias [768] float32\n",
      "transformer.h.6.ln_2.weight [768] float32\n",
      "transformer.h.6.ln_2.bias [768] float32\n",
      "transformer.h.6.mlp.c_fc.weight [3072, 768] float32\n",
      "transformer.h.6.mlp.c_fc.bias [3072] float32\n",
      "transformer.h.6.mlp.c_proj.weight [768, 3072] float32\n",
      "transformer.h.6.mlp.c_proj.bias [768] float32\n",
      "transformer.h.7.ln_1.weight [768] float32\n",
      "transformer.h.7.ln_1.bias [768] float32\n",
      "transformer.h.7.attn.c_attn.weight [2304, 768] float32\n",
      "transformer.h.7.attn.c_attn.bias [2304] float32\n",
      "transformer.h.7.attn.c_proj.weight [768, 768] float32\n",
      "transformer.h.7.attn.c_proj.bias [768] float32\n",
      "transformer.h.7.ln_2.weight [768] float32\n",
      "transformer.h.7.ln_2.bias [768] float32\n",
      "transformer.h.7.mlp.c_fc.weight [3072, 768] float32\n",
      "transformer.h.7.mlp.c_fc.bias [3072] float32\n",
      "transformer.h.7.mlp.c_proj.weight [768, 3072] float32\n",
      "transformer.h.7.mlp.c_proj.bias [768] float32\n",
      "transformer.h.8.ln_1.weight [768] float32\n",
      "transformer.h.8.ln_1.bias [768] float32\n",
      "transformer.h.8.attn.c_attn.weight [2304, 768] float32\n",
      "transformer.h.8.attn.c_attn.bias [2304] float32\n",
      "transformer.h.8.attn.c_proj.weight [768, 768] float32\n",
      "transformer.h.8.attn.c_proj.bias [768] float32\n",
      "transformer.h.8.ln_2.weight [768] float32\n",
      "transformer.h.8.ln_2.bias [768] float32\n",
      "transformer.h.8.mlp.c_fc.weight [3072, 768] float32\n",
      "transformer.h.8.mlp.c_fc.bias [3072] float32\n",
      "transformer.h.8.mlp.c_proj.weight [768, 3072] float32\n",
      "transformer.h.8.mlp.c_proj.bias [768] float32\n",
      "transformer.h.9.ln_1.weight [768] float32\n",
      "transformer.h.9.ln_1.bias [768] float32\n",
      "transformer.h.9.attn.c_attn.weight [2304, 768] float32\n",
      "transformer.h.9.attn.c_attn.bias [2304] float32\n",
      "transformer.h.9.attn.c_proj.weight [768, 768] float32\n",
      "transformer.h.9.attn.c_proj.bias [768] float32\n",
      "transformer.h.9.ln_2.weight [768] float32\n",
      "transformer.h.9.ln_2.bias [768] float32\n",
      "transformer.h.9.mlp.c_fc.weight [3072, 768] float32\n",
      "transformer.h.9.mlp.c_fc.bias [3072] float32\n",
      "transformer.h.9.mlp.c_proj.weight [768, 3072] float32\n",
      "transformer.h.9.mlp.c_proj.bias [768] float32\n",
      "transformer.h.10.ln_1.weight [768] float32\n",
      "transformer.h.10.ln_1.bias [768] float32\n",
      "transformer.h.10.attn.c_attn.weight [2304, 768] float32\n",
      "transformer.h.10.attn.c_attn.bias [2304] float32\n",
      "transformer.h.10.attn.c_proj.weight [768, 768] float32\n",
      "transformer.h.10.attn.c_proj.bias [768] float32\n",
      "transformer.h.10.ln_2.weight [768] float32\n",
      "transformer.h.10.ln_2.bias [768] float32\n",
      "transformer.h.10.mlp.c_fc.weight [3072, 768] float32\n",
      "transformer.h.10.mlp.c_fc.bias [3072] float32\n",
      "transformer.h.10.mlp.c_proj.weight [768, 3072] float32\n",
      "transformer.h.10.mlp.c_proj.bias [768] float32\n",
      "transformer.h.11.ln_1.weight [768] float32\n",
      "transformer.h.11.ln_1.bias [768] float32\n",
      "transformer.h.11.attn.c_attn.weight [2304, 768] float32\n",
      "transformer.h.11.attn.c_attn.bias [2304] float32\n",
      "transformer.h.11.attn.c_proj.weight [768, 768] float32\n",
      "transformer.h.11.attn.c_proj.bias [768] float32\n",
      "transformer.h.11.ln_2.weight [768] float32\n",
      "transformer.h.11.ln_2.bias [768] float32\n",
      "transformer.h.11.mlp.c_fc.weight [3072, 768] float32\n",
      "transformer.h.11.mlp.c_fc.bias [3072] float32\n",
      "transformer.h.11.mlp.c_proj.weight [768, 3072] float32\n",
      "transformer.h.11.mlp.c_proj.bias [768] float32\n",
      "transformer.ln_f.weight [768] float32\n",
      "transformer.ln_f.bias [768] float32\n",
      "lm_head.weight [50257, 768] float32\n"
     ]
    }
   ],
   "source": [
    "model_info = MODELS[\"gpt2\"]\n",
    "config = model_info.config.from_dict(MODEL_PRESETS[\"gpt2\"])\n",
    "model = model_info.model(config)\n",
    "mod, named_params = model.export_tvm(\n",
    "    spec=model.get_default_spec(),  # type: ignore\n",
    ")\n",
    "# mod.show(black_format=False)\n",
    "for name, param in named_params:\n",
    "    print(name, param.shape, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2Model.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2MLP(\n",
       "  (c_fc): Conv1D()\n",
       "  (c_proj): Conv1D()\n",
       "  (act): NewGELUActivation()\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_mlp = model.h[0].mlp\n",
    "hf_mlp.dropout.p = 0\n",
    "hf_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand((1, 768), dtype=torch.float32)\n",
    "\n",
    "c_fc_weight = hf_mlp.c_fc.weight.data.numpy()\n",
    "c_fc_bias = hf_mlp.c_fc.bias.data.numpy()\n",
    "c_proj_weight = hf_mlp.c_proj.weight.data.numpy()\n",
    "c_proj_bias = hf_mlp.c_proj.bias.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = hf_mlp.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('c_fc.weight', Tensor([3072, 768], \"float32\")),\n",
       "             ('c_fc.bias', Tensor([3072], \"float32\")),\n",
       "             ('c_proj.weight', Tensor([768, 3072], \"float32\")),\n",
       "             ('c_proj.bias', Tensor([768], \"float32\"))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlc_chat.compiler.model.gpt2 import gpt2_model\n",
    "from tvm.relax.frontend.nn import spec\n",
    "\n",
    "mlp = gpt2_model.GPT2MLP(config)\n",
    "state_dict = mlp.state_dict()\n",
    "state_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvm.relax.frontend import nn\n",
    "from tvm.relax.frontend.nn import Tensor, op\n",
    "import numpy as np\n",
    "\n",
    "state_dict['c_fc.weight'].data = c_fc_weight.T\n",
    "state_dict['c_fc.bias'].data = c_fc_bias\n",
    "state_dict['c_proj.weight'].data = c_proj_weight.T\n",
    "state_dict['c_proj.bias'].data = c_proj_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_spec = {\"forward\": {\"hidden_states\": spec.Tensor([1, 768], dtype=\"float32\")}}\n",
    "torch_mlp = mlp.jit(spec=mlp_spec, debug=True)\n",
    "y2 = torch_mlp[\"forward\"](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(y1, y2, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Attention(\n",
       "  (c_attn): Conv1D()\n",
       "  (c_proj): Conv1D()\n",
       "  (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "  (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_attn = model.h[0].attn\n",
    "hf_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 1, 768), dtype=torch.float32)\n",
    "\n",
    "c_attn_weight = hf_attn.c_attn.weight.data.numpy()\n",
    "c_attn_bias = hf_attn.c_attn.bias.data.numpy()\n",
    "c_proj_weight = hf_attn.c_proj.weight.data.numpy()\n",
    "c_proj_bias = hf_attn.c_proj.bias.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 7.6107e-01,  1.2312e+01, -1.1915e+00, -1.2172e+00,  9.3915e-01,\n",
       "            3.7703e-01, -6.1495e+00,  6.1346e+00,  4.1892e+00,  7.4580e-02,\n",
       "            7.4806e+00, -4.7293e-01, -5.4205e-01,  1.7085e+00,  2.2852e+00,\n",
       "           -3.9748e+00,  7.6214e+00, -2.5467e+00, -7.7637e-01,  1.7110e+00,\n",
       "           -1.8825e+00, -2.3129e+00,  8.2723e-01,  4.3244e+00,  1.5845e+00,\n",
       "           -9.9023e-01, -4.8893e+00,  1.5530e+00,  2.8631e-01,  5.8775e-01,\n",
       "            1.2878e+00, -9.9392e-01,  2.1662e+00, -3.3726e-01,  8.9550e-01,\n",
       "           -1.0003e+01,  2.0521e+00, -7.2917e-01, -8.7611e-01,  1.0263e+00,\n",
       "           -3.3785e-01,  2.7569e+00,  2.7468e+00,  2.1164e-01, -3.9742e+00,\n",
       "            4.4884e-01,  1.9716e-01,  2.3247e+00,  9.9650e+00,  1.1739e+01,\n",
       "            1.2716e+00,  3.4024e-01,  8.5342e+00,  1.8538e-01, -7.6792e-01,\n",
       "            6.4777e+00,  3.0169e+00, -6.8197e+00, -5.0132e-01,  6.3051e+00,\n",
       "            1.6772e+00,  3.1065e+00, -4.2201e-01,  1.9741e+00,  6.9222e+01,\n",
       "            1.1794e-01,  4.5909e-01,  4.7454e+00,  6.0172e+00, -1.3609e+00,\n",
       "            1.8808e+00,  7.6261e+00,  1.2308e+00, -6.5331e-01,  8.8870e-02,\n",
       "           -1.4588e+00,  5.8129e+00,  5.2009e+00,  6.4870e+00, -4.4308e-01,\n",
       "           -6.1105e-01, -1.7873e+00, -4.4436e-01,  2.2864e+00, -9.0166e-01,\n",
       "           -3.6064e+00, -2.9926e+00, -2.7680e+01, -3.0747e-01, -5.7172e-01,\n",
       "           -1.7664e+00, -4.3232e-01,  1.5318e+00, -8.1494e-01, -1.0866e-01,\n",
       "           -1.1125e-01, -2.7056e+00, -3.3952e-01,  1.8402e+00,  4.2850e+00,\n",
       "            5.3813e-01,  1.0248e+01,  6.3650e+00, -4.0809e+00, -1.3607e+00,\n",
       "            5.3026e-02, -9.9062e-01,  8.3123e-02, -7.7563e-01,  3.9336e-01,\n",
       "           -9.5717e-01, -1.7060e-01, -9.8418e-01, -9.7818e-01,  1.0123e+00,\n",
       "           -1.2096e+00,  3.8919e+00,  2.1452e+00, -3.2780e-01, -6.5651e-01,\n",
       "           -1.2974e-01, -4.7635e-01, -9.2998e-01, -4.2230e-01,  3.9938e-01,\n",
       "            6.5930e-01,  4.0414e-01, -1.3042e-01,  3.3733e+00, -1.3623e+01,\n",
       "           -8.9329e-01, -2.4340e-01, -1.0260e+01,  2.7022e-01,  1.0617e+00,\n",
       "            3.4748e-02,  5.8063e-01,  5.7962e+00, -2.7554e-01,  4.2820e+00,\n",
       "            1.7812e+00, -1.5487e+00,  2.5551e+00,  6.0312e-01,  7.1536e-02,\n",
       "            1.7728e+00, -4.1748e-02,  7.6855e-02, -1.7964e+00,  9.3846e+00,\n",
       "            5.8248e-01, -2.4941e-01,  2.5109e+00, -6.6079e+00, -1.3061e+00,\n",
       "           -1.6951e-01, -2.3227e+00,  4.3789e-01,  4.3652e-02, -2.9050e-01,\n",
       "           -5.1573e+00, -1.2021e+00,  5.9040e-01,  1.3962e+00, -1.0624e+00,\n",
       "           -1.4995e+00,  4.1673e-01, -1.2838e-01, -1.2831e+00,  2.7641e+00,\n",
       "           -4.6973e-01,  1.7238e+00,  1.3077e+00,  2.2786e+00, -4.0674e+00,\n",
       "           -3.0377e+00,  3.5114e+00, -1.7617e-01,  8.8134e+00, -2.3047e+00,\n",
       "           -1.8064e+00,  1.5713e-01,  1.1037e+00,  5.0340e+00,  3.6415e-01,\n",
       "           -9.6650e-01, -6.9864e-01,  4.9797e+00,  1.1882e+00, -3.5072e-01,\n",
       "           -1.2555e+00,  2.1336e+00, -1.0674e+00,  1.2233e+00,  8.9175e-01,\n",
       "           -1.2399e+01,  3.8255e-01,  2.7248e+00, -3.7096e-01,  4.6040e+00,\n",
       "            2.3274e+00,  7.4143e-01,  1.3536e+00,  1.2133e+00, -1.3089e+00,\n",
       "            6.0956e-01, -4.3403e-01,  4.0696e-01,  6.3790e+00, -7.6792e-01,\n",
       "            3.7680e-01, -6.0492e+00,  6.0692e-01,  9.9307e+00, -3.3043e-01,\n",
       "            8.5654e-02, -8.9446e+00,  9.2487e-01,  3.3482e+00, -7.0878e-01,\n",
       "            1.5621e+00,  3.9460e-01,  4.7085e-01,  2.9289e+00,  6.2655e+00,\n",
       "            8.0744e-01,  8.1925e-01, -1.4584e+00,  5.2436e-01, -1.6395e+00,\n",
       "            4.8049e-01, -3.6795e+00, -2.1793e+00,  6.5246e-02, -3.0826e-01,\n",
       "            4.0445e+00, -5.4021e-01, -1.3206e-01, -2.1020e+00, -5.8436e-01,\n",
       "           -7.5555e+00, -2.1583e+00, -2.0926e+00, -1.1330e+00, -3.3629e-01,\n",
       "           -1.2990e+00, -8.9142e-01, -6.1505e+00, -1.3037e+00, -1.9336e+00,\n",
       "            3.4872e+00,  1.2016e+01, -3.6313e-01,  1.0460e+01, -6.8198e-01,\n",
       "            3.8035e+00,  5.4394e-01,  9.3360e-01,  8.3046e-01, -3.3161e+00,\n",
       "            1.0583e-01,  1.5571e+00, -1.4822e-01, -5.9101e-01,  1.4359e+00,\n",
       "            2.3002e+00,  8.6464e+01,  4.3172e+00,  1.8691e+00, -6.8792e+00,\n",
       "            1.6259e+01, -2.3879e-01, -8.6396e-01, -3.2057e+00, -5.8758e+00,\n",
       "           -5.0146e-01,  5.3265e+00, -2.2086e+00, -1.0451e-01,  7.5047e-02,\n",
       "           -8.0961e-01, -2.0802e+00, -3.8046e-01,  3.7405e+00,  2.0173e+00,\n",
       "           -1.8232e-01,  1.0017e+00,  2.1635e-01, -1.2059e+02, -2.7371e+00,\n",
       "           -4.0167e-01,  1.3847e+00, -1.1540e+00, -4.0259e-01, -4.3834e+00,\n",
       "            1.4844e+00, -3.3823e-01,  5.5361e-01,  1.6407e+00, -1.0024e+00,\n",
       "            1.3496e+01, -7.6401e-01,  1.1266e+01, -4.3330e-02,  1.0067e+00,\n",
       "            7.6715e-01,  1.5254e+00, -8.6778e-01, -2.5238e+00,  6.5797e-02,\n",
       "           -1.0314e+00,  1.0298e+01,  8.4012e-02,  5.2787e-01,  1.2830e+00,\n",
       "           -2.8039e+00,  4.8099e+00, -7.0683e-01, -1.8494e+00, -1.3644e+00,\n",
       "            5.8180e+00, -9.3521e-02,  1.8614e-01,  4.2550e+00,  4.5753e+00,\n",
       "            2.8223e-01, -1.4227e+02, -1.0472e+00, -7.6116e+00,  4.1180e-01,\n",
       "            1.2683e+00,  8.9215e-02,  1.3982e+00,  5.1412e-01,  3.5044e-01,\n",
       "            7.4600e-01,  8.4649e-01,  2.2635e-01, -2.0218e+00,  1.2795e+01,\n",
       "           -2.7680e-01,  1.0457e-01,  8.3494e-03,  4.8082e-01,  9.6654e+00,\n",
       "            1.8488e-01, -5.8961e-01,  8.8487e-01,  5.5613e-01, -3.1198e+00,\n",
       "            2.2083e-01,  1.6900e+01,  5.3767e-01,  4.8252e+00,  7.4683e-01,\n",
       "            1.7635e+00, -7.7062e+00,  1.0806e+00,  6.4287e-01,  6.3956e-02,\n",
       "           -5.4718e-01,  3.0491e-01,  7.4705e-01, -1.2305e+00,  4.7566e-01,\n",
       "           -2.3167e+00,  2.9883e-01,  1.1150e-01,  1.5244e+00,  8.1318e-02,\n",
       "            2.2084e+00,  1.0820e+00, -1.6677e+00,  1.2655e+02,  1.0161e+00,\n",
       "            6.8457e+00,  7.5202e+00, -6.7633e+00,  6.3498e-01,  1.9349e-02,\n",
       "            2.1839e-01,  2.2767e+00, -1.7031e+00, -1.6599e+01,  3.0730e+00,\n",
       "            1.2785e+01,  1.0978e+00,  1.6876e-01, -7.5219e-01,  2.9792e+00,\n",
       "            6.0896e-01,  1.7392e+00,  2.4527e+00,  2.3436e+01,  1.4790e+00,\n",
       "           -4.7567e-01,  3.4299e+00, -9.4671e-01, -1.0620e-01, -7.8908e-01,\n",
       "           -1.8462e-01,  1.6377e+00, -2.2566e+00, -1.6069e+00, -6.3417e-01,\n",
       "           -1.0284e+00, -1.3608e+00, -2.7217e-01, -1.1816e+00,  2.0313e-01,\n",
       "           -1.9420e+00,  4.9993e-01,  3.2343e-01, -7.5035e-01, -5.8762e+00,\n",
       "           -2.5014e+00, -2.0933e+00, -3.5796e+00,  6.4903e+00,  6.6449e-01,\n",
       "            1.5760e+01,  8.6189e+00, -1.3242e+00,  9.4352e-01,  1.5053e-01,\n",
       "           -4.6376e-01,  7.6349e-01,  1.3915e+00, -6.1417e+00,  1.9878e-01,\n",
       "           -1.5530e+00,  3.4994e+00,  2.5395e+00,  6.4320e-01, -1.7117e+00,\n",
       "           -1.2400e+00, -1.2560e+00, -1.3574e+00, -3.6914e-01, -3.1563e+00,\n",
       "           -7.0246e-01,  1.8744e+00,  6.8384e+00, -4.2514e+00,  6.4581e+00,\n",
       "           -1.1755e+00, -1.6737e+00, -1.3395e+02, -6.8131e-01, -3.0636e-01,\n",
       "           -2.0853e+00, -8.5591e-01, -1.3692e-01,  1.2907e+01, -1.7337e+00,\n",
       "            1.8899e+01,  1.2275e+00,  1.1334e+00, -2.8824e-01, -1.2476e+01,\n",
       "            1.8745e+00, -6.9915e-01,  3.6271e-01, -1.3868e-01,  1.7064e+01,\n",
       "           -4.5454e+00, -6.1373e-01,  1.5910e+00, -6.5031e-01,  1.2268e+00,\n",
       "            7.1353e-01, -1.6679e+00, -5.6608e-01, -7.1944e-01,  1.0294e+00,\n",
       "           -9.7468e-01, -4.9792e-01, -7.8751e-01,  2.1168e-01,  4.2713e+00,\n",
       "            1.5005e+01,  7.3399e+00,  3.7054e-01,  5.3412e-01, -4.7776e+00,\n",
       "            9.7958e-02, -6.5402e-01, -2.9221e+00, -5.3364e+00,  5.7126e-01,\n",
       "            5.0434e-01,  3.0480e+00,  7.1125e-01,  6.8494e-01,  9.6917e-01,\n",
       "           -1.4065e+00,  1.5060e+00,  1.0241e+01, -5.4756e+00,  8.7838e-01,\n",
       "            5.3005e-01,  8.3061e-01,  7.3570e+00,  4.9443e+00,  8.1762e+00,\n",
       "            3.1233e+00,  1.3819e+00,  1.4812e+00, -5.0124e+00, -3.7261e-01,\n",
       "           -4.1060e-01,  4.1408e+00, -9.7752e-02, -1.2666e+00,  1.5005e+00,\n",
       "            3.4268e+00,  3.0166e-01,  4.2202e+00, -3.6544e-01, -3.6617e-02,\n",
       "           -1.2481e+00, -7.3879e-01,  3.2355e-01,  7.3132e-02, -1.0430e-01,\n",
       "            1.6384e+00, -5.2174e+00, -2.1586e+00, -5.0908e+00, -1.1534e-01,\n",
       "           -7.9057e-01, -1.8925e+00, -7.4588e-01,  7.9560e-01,  5.2508e-01,\n",
       "           -7.9503e-01,  2.6411e-01, -3.3077e-01, -6.8530e-01,  1.6571e+00,\n",
       "            5.7717e+00, -1.5174e+00, -7.1325e-01, -1.6905e+00,  1.9243e+00,\n",
       "            1.6437e+00, -1.7375e+00, -2.3908e-01,  1.0985e+00,  3.1772e+00,\n",
       "            1.2034e+00,  1.4739e+00, -1.8762e+00, -1.4611e+00,  1.9314e+01,\n",
       "            5.1363e-01,  9.2224e-01,  1.5415e+00,  1.0570e+00,  3.2004e+00,\n",
       "           -3.9853e-01, -1.0030e+00, -1.0524e+00,  2.7434e+00,  7.5096e-01,\n",
       "           -9.1115e+00,  1.0983e+01, -2.3671e-01,  1.9712e+00,  3.2718e+00,\n",
       "           -1.5059e+00,  2.8886e+00,  9.2504e-01, -1.3052e+00, -1.4853e-02,\n",
       "           -9.5427e+00,  8.5364e-01,  1.6823e+00,  1.4653e+01, -1.3780e+00,\n",
       "           -1.9577e+00,  3.5192e+00,  5.0953e-02,  7.1925e-01, -1.3276e+00,\n",
       "           -5.8693e-01,  2.4435e-01,  6.7541e+00, -3.9664e+00,  9.1332e-01,\n",
       "            5.8094e-01,  1.2503e-01, -1.3038e+00,  1.0159e+00, -2.1296e+00,\n",
       "           -9.3632e-01, -1.7948e+00, -6.2268e-01,  6.4718e-01, -1.4765e-01,\n",
       "            2.1924e+00,  3.6625e-01,  5.2284e-01, -4.4448e+00,  1.4330e+00,\n",
       "           -5.6632e+00, -9.6063e-01,  6.3932e-02, -3.7494e+00, -1.3958e+00,\n",
       "           -6.0198e-01,  5.4722e+00, -2.2120e-01,  6.9494e-01, -1.5332e+00,\n",
       "           -1.0460e+01, -1.2368e+00, -2.4250e+00,  1.3529e+00,  8.5118e+00,\n",
       "           -5.5881e-02, -4.7809e-01,  9.3497e+00, -6.0993e+00, -3.3343e+00,\n",
       "           -1.0113e+00, -1.4673e+00, -2.0203e+00,  1.1815e-01, -2.3085e+00,\n",
       "           -2.8788e+00,  2.0804e+00, -1.8278e+00,  1.9983e+00, -6.8609e-01,\n",
       "            1.5033e+01, -5.2494e-02, -2.4954e-01, -1.2154e+00,  1.1816e+00,\n",
       "            1.9430e+01, -2.8744e-01, -1.3879e+01, -1.1547e+01,  1.6407e+01,\n",
       "            1.5839e+00,  8.7195e-01,  3.0263e+00, -2.1211e+00, -1.3491e+00,\n",
       "           -5.4744e-01, -9.0419e+00, -2.2650e+00, -9.7467e-01,  4.5297e-02,\n",
       "            4.1192e+00, -9.0956e-01,  1.0493e+00, -4.8697e-01, -1.7792e+00,\n",
       "           -2.1100e+00, -1.1328e+00,  3.9782e-01,  1.4539e+00,  3.9116e-01,\n",
       "            1.8781e+00,  7.7190e-01,  5.7748e+00, -4.1158e-01, -9.9056e-02,\n",
       "            1.1542e+01, -4.9235e-01,  3.2957e-01,  2.0872e-02,  6.8530e+00,\n",
       "            5.5114e-01, -1.7084e+00,  1.7085e+00, -2.3616e-01, -2.4090e+01,\n",
       "           -2.9766e+00,  2.2482e+00,  1.5507e+00, -3.9535e-01,  8.8181e-01,\n",
       "            8.6723e+00, -1.2570e+00,  3.6022e-01,  5.7476e+00,  5.8376e-01,\n",
       "           -1.0795e+00,  1.2171e+00, -2.5553e-01, -1.1952e+00, -8.7518e-01,\n",
       "           -4.1820e-01, -1.1920e+00, -4.4984e-01, -1.5857e+00,  6.7080e+00,\n",
       "           -1.6497e+00, -6.4625e-01,  1.0685e+00, -1.2011e+00, -2.1867e-01,\n",
       "           -1.9657e+00, -1.1041e+01,  1.6017e+00,  1.9443e+00,  4.0854e+00,\n",
       "            1.1366e+00,  5.6220e-01, -5.4788e-01, -1.2535e+00, -7.8620e-01,\n",
       "            6.2566e-01, -5.0684e-01, -4.1648e+00, -1.8582e+00,  3.3945e-01,\n",
       "            8.6016e+00, -3.0814e+00, -8.8093e-01, -1.5959e+00,  5.5398e+00,\n",
       "            2.0979e+00, -3.4985e+00,  2.7931e+00, -1.7804e+00,  5.8827e-01,\n",
       "            1.5739e+00,  1.0603e+00, -1.1679e-01, -4.3337e+00, -5.9079e-01,\n",
       "            1.1802e+00,  3.0671e-01,  1.6572e+00,  4.0288e-01, -9.8082e+00,\n",
       "            1.1714e-01,  1.5709e+00, -3.5507e-01, -5.5266e-01,  8.3765e-01,\n",
       "            9.4477e+00,  1.3391e+00, -2.4503e+00, -2.2998e+00,  1.3234e+00,\n",
       "            3.9315e-03,  3.0144e+00, -3.6946e-01, -1.8133e+00, -3.3552e+00,\n",
       "           -3.5641e+00,  2.0296e+00, -1.3069e+00, -5.1835e-01,  6.2477e-01,\n",
       "            6.2987e-01, -1.0558e+00, -1.7568e+00, -4.3089e-01, -1.6856e+00,\n",
       "            2.3376e+00,  3.0584e-02,  9.3390e-01]]], grad_fn=<ViewBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.ones((1, 1, 1, 1), dtype=torch.float32)\n",
    "\n",
    "y1 = hf_attn.forward(x, attention_mask=mask)\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = gpt2_model.GPT2Attention(config)\n",
    "state_dict = attn.state_dict()\n",
    "\n",
    "state_dict['c_attn.weight'].data = c_attn_weight.T\n",
    "state_dict['c_attn.bias'].data = c_attn_bias\n",
    "state_dict['c_proj.weight'].data = c_proj_weight.T\n",
    "state_dict['c_proj.bias'].data = c_proj_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_spec = {\"forward\": {\"hidden_states\": spec.Tensor([1, 1, 768], dtype=\"float32\"), \"attention_mask\": spec.Tensor([1, 1, 1, 1], dtype=\"float32\"), \"total_seq_len\": int}}\n",
    "torch_attn = attn.jit(spec=attn_spec, debug=True)\n",
    "\n",
    "y2 = torch_attn[\"forward\"](x, mask, 1)\n",
    "assert torch.allclose(y1[0], y2, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_block = model.h[0]\n",
    "hf_state_dict = hf_block.state_dict()\n",
    "\n",
    "for k, v in hf_state_dict.items():\n",
    "    for name in [\"attn.c_attn\", \"attn.c_proj\", \"mlp.c_proj\", \"mlp.c_fc\"]:\n",
    "        if name in k and 'weight' in k:\n",
    "            hf_state_dict[k] = v.T\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = gpt2_model.GPT2Block(config)\n",
    "block.load_state_dict(hf_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1, 1, 768), dtype=torch.float32)\n",
    "mask = torch.ones((1, 1, 1, 1), dtype=torch.float32)\n",
    "\n",
    "y1 = hf_block(x, attention_mask=mask)\n",
    "\n",
    "block_spec = {\"forward\": {\"hidden_states\": spec.Tensor([1, 1, 768], dtype=\"float32\"), \"attention_mask\": spec.Tensor([1, 1, 1, 1], dtype=\"float32\"), \"total_seq_len\": int}}\n",
    "torch_block = block.jit(spec=block_spec, debug=True)\n",
    "\n",
    "y2 = torch_block[\"forward\"](x, mask, 1)\n",
    "assert torch.allclose(y1[0], y2, 0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-chat-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
